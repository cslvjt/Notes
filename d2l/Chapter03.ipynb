{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 线性回归\n",
    "<div align=center>\n",
    "<img src=./img/chapter03/1.jpg >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss2.178056\n",
      "epoch2,loss0.282730\n",
      "epoch3,loss0.036742\n",
      "epoch4,loss0.004823\n",
      "epoch5,loss0.000672\n",
      "epoch6,loss0.000129\n",
      "epoch7,loss0.000058\n",
      "epoch8,loss0.000049\n",
      "epoch9,loss0.000048\n",
      "epoch10,loss0.000048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "def synthetic_data(w,b,num_examples):\n",
    "    x=torch.normal(0,1,(num_examples,len(w)))\n",
    "    y=torch.matmul(x,w)+b\n",
    "    y+=torch.normal(0,0.01,y.shape)\n",
    "    return x,y.reshape((-1,1))\n",
    "\n",
    "def data_iter(batch_size, features, labels): \n",
    "    num_examples = len(features) \n",
    "    indices = list(range(num_examples)) \n",
    "    # 这些样本是随机读取的，没有特定的顺序 \n",
    "    random.shuffle(indices) \n",
    "    for i in range(0, num_examples, batch_size): \n",
    "        batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) \n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=torch.tensor(4.2)\n",
    "\n",
    "def liner_net(X,w,b):\n",
    "    return torch.matmul(X,w)+b\n",
    "def loss(y_hat,y):\n",
    "    return(y_hat-y.reshape(y_hat.shape))**2/2\n",
    "def sgd(params,lr,batchszie):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param-=lr*param.grad/batchszie\n",
    "            param.grad.zero_()\n",
    "def train():\n",
    "    lr=0.01\n",
    "    num_epochs=10\n",
    "    w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) \n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    batchsize=10\n",
    "    feature,labels=synthetic_data(true_w,true_b,1000)\n",
    "    for epoch in range(num_epochs):\n",
    "        for X,y in data_iter(batchsize,feature,labels):\n",
    "            l=loss(liner_net(X,w,b),y)\n",
    "            l.sum().backward()\n",
    "            sgd([w,b],lr,batchsize)\n",
    "        print(f'epoch{epoch+1},loss{loss(net(feature,w,b),labels).mean():f}')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 logstic回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ softmax回归和线性回归有什么区别？\n",
    "\n",
    "线性回归是解决预测问题，输出空间是连续的数值。而softmax回归是分类问题，输出空间是离散的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础概念\n",
    "1. 独热编码(one-hot coding)：独热编码是一个向量，它的分量和类别一样多。类别相应的分量为1，其他分量为0。例如：(1; 0; 0)对应于 “猫”、(0; 1; 0)对应于“鸡”、(0; 0; 1)对应于“狗。\n",
    "2. softmax函数：$\\hat{y_j}=\\frac{exp(o_j)}{\\sum_k exp(o_j)}$\n",
    "3. 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回 归是一个线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/chapter03/2.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "def load_data(batchsize,resize=None):\n",
    "    trans=[transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans=transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(\n",
    "        root=\"./data\",train=True,transform=trans,download=False\n",
    "    )\n",
    "    mnist_test=torchvision.datasets.FashionMNIST(\n",
    "        root=\"./data\",train=False,transform=trans,download=False\n",
    "    )\n",
    "    return (data.DataLoader(mnist_train, batchsize, shuffle=True, num_workers=1), \n",
    "            data.DataLoader(mnist_test, batchsize, shuffle=False, num_workers=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator: #@save \"\"\"在`n`个变量上累加。\"\"\" \n",
    "    def __init__(self, n): \n",
    "        self.data = [0.0] * n \n",
    "    def add(self, *args): \n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)] \n",
    "    def reset(self): \n",
    "        self.data = [0.0] * len(self.data) \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp=torch.exp(X)\n",
    "    partition=X_exp.sum(1,keepdim=True)\n",
    "    return X_exp/partition #广播机制\n",
    "def softmax_net(X,w,b):\n",
    "    return softmax(torch.matmul(X.reshape((-1,w.shape[0])),w)+b)\n",
    "def cross_entroy(y_hat,y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])\n",
    "def update(w,b,lr,batchsize):\n",
    "    return d2l.sgd([w,b],lr,batchsize)\n",
    "def accuracy(y_hat,y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: \n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y \n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "def evaluate_accuracy(w,b,data_iter):\n",
    "    metric=Accumulator(2)\n",
    "    for x,y in data_iter:\n",
    "        metric.add(accuracy(net(x,w,b),y),y.numel())\n",
    "    return metric[0]/metric[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    num_inputs=784\n",
    "    num_outputs=10\n",
    "    W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) \n",
    "    b = torch.zeros(num_outputs, requires_grad=True)\n",
    "    batchsize=10\n",
    "    num_epochs=10\n",
    "    train_iter,test_iter=load_data(batchsize=batchsize)\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_iter:\n",
    "            y_hat=softmax_net(x,W,b)\n",
    "            l=cross_entroy(y_hat,y)\n",
    "            l.sum().backward()\n",
    "            update(W,b,lr=0.01,batchsize=batchsize)\n",
    "        test_acc=evaluate_accuracy(W,b,test_iter)\n",
    "        print(f\"epoch:{epoch+1},test_acc:{test_acc:f}\")\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fbcb25ffd1e704b77e766f8f15869059d849d69d6a71e37721ac89c2bbcb90f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
